{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65-gcGPBoy4K"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D679oMEoYsG"
      },
      "outputs": [],
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\" pygame matplotlib ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "16k8nmQuod_P"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from IPython.display import HTML, Javascript, display\n",
        "\n",
        "import base64\n",
        "import io\n",
        "from PIL import Image\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7peCCmyno2Ys"
      },
      "source": [
        "# CEM algorithm implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p1aG6oDEogUo"
      },
      "outputs": [],
      "source": [
        "class CEM:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        bounds: np.ndarray,\n",
        "        population_size: int = 500,\n",
        "        elite_frac: float = 0.1,\n",
        "        max_iters: int = 50,\n",
        "        smoothing: float = 0.7,\n",
        "        seed: int = None,\n",
        "    ):\n",
        "        self.dim = dim\n",
        "        self.bounds = bounds\n",
        "        self.pop_size = population_size\n",
        "        self.n_elite = max(1, int(np.ceil(elite_frac * population_size)))\n",
        "        self.max_iters = max_iters\n",
        "        self.alpha = smoothing\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.best_obj_history = []\n",
        "\n",
        "    def optimize(self, x_init: np.ndarray, objective):\n",
        "        mu = x_init.copy()\n",
        "        sigma = (self.bounds[1] - self.bounds[0]) / 2.0\n",
        "\n",
        "        self.best_obj_history = []\n",
        "\n",
        "        for i in range(self.max_iters):\n",
        "            pass\n",
        "\n",
        "            # 1. random sample x and clip with bounds\n",
        "            samples = np.clip(self.rng.normal(mu, sigma, (self.pop_size, self.dim)), self.bounds[0], self.bounds[1])\n",
        "\n",
        "            # 2. query y values from sampled x\n",
        "            ys = objective(samples)\n",
        "\n",
        "            # 3. get top k minimum (x,y)\n",
        "            elite_idxs = ys.argsort()[:self.n_elite]\n",
        "            elite_samples = samples[elite_idxs]\n",
        "\n",
        "            # 3. calculate mean, std from top k samples\n",
        "            new_mu = elite_samples.mean(axis=0)\n",
        "            new_sigma = elite_samples.std(axis=0)\n",
        "\n",
        "            # 3. update mean and sigma\n",
        "            mu = self.alpha * mu + (1 - self.alpha) * new_mu\n",
        "            sigma = self.alpha * sigma + (1 - self.alpha) * new_sigma\n",
        "\n",
        "            # [optional] For debug, log best y values and mean y values, etc ..\n",
        "            best_y = ys[elite_idxs[0]]\n",
        "            mean_y = ys.mean()\n",
        "            self.best_obj_history.append(best_y)\n",
        "            print(f\"Iteration: {i} / best_y: {best_y} / mean_y: {mean_y}\")\n",
        "\n",
        "        # return final mean\n",
        "        return mu\n",
        "\n",
        "    def visualize(self):\n",
        "        plt.figure()\n",
        "        plt.plot(np.arange(1, len(self.best_obj_history) + 1), self.best_obj_history, marker='o')\n",
        "        plt.title(\"Best Value per Iteration\")\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Objective (Lower is Better)\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0TMMS9cn7Ah"
      },
      "source": [
        "# Objective function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qs9SiMezp_8d"
      },
      "outputs": [],
      "source": [
        "def objective_vec(train_env, batch: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generic vectorized discrete-action objective for CEM (to be MINIMIZED).\n",
        "    Works for CartPole (action_dim=2) or LunarLander (action_dim=4) or any other discrete gym env.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_env : VectorEnv\n",
        "        A vectorized gymnasium environment.\n",
        "    batch : np.ndarray, shape (N, D)\n",
        "        Population of parameter vectors. Each θ has length\n",
        "            obs_dim * action_dim  +  action_dim\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray, shape (N,)\n",
        "        Negative average reward per candidate (CEM will minimize).\n",
        "    \"\"\"\n",
        "    N, D = batch.shape\n",
        "    obs_dim = train_env.single_observation_space.shape[0]\n",
        "    action_dim = train_env.single_action_space.n\n",
        "\n",
        "    # sanity check\n",
        "    assert D == obs_dim * action_dim + action_dim, (\n",
        "        f\"Expected θ to be length {obs_dim*action_dim + action_dim}, \"\n",
        "        f\"got {D}\"\n",
        "    )\n",
        "\n",
        "    results = np.zeros(N, dtype=float)\n",
        "\n",
        "    for i in range(N):\n",
        "        theta = batch[i]\n",
        "        # 1. unpack θ → W, b\n",
        "        W = theta[: obs_dim * action_dim].reshape(obs_dim, action_dim)\n",
        "        b = theta[obs_dim * action_dim :]\n",
        "\n",
        "        # 2. get sum of rewards with policy and env\n",
        "        # Note: sum of rewards should be maximized, but CEM works to minimize objective. So we should change the sign.\n",
        "        obs, _ = train_env.reset()\n",
        "        done_mask = np.zeros(train_env.num_envs, dtype=bool)\n",
        "        cum_rewards = np.zeros(train_env.num_envs, dtype=float)\n",
        "\n",
        "        # run until each sub-env ends\n",
        "        while not done_mask.all():\n",
        "            # compute per‐env logits and pick argmax\n",
        "            logits = obs.dot(W) + b        # (num_envs, action_dim)\n",
        "            actions = np.argmax(logits, axis=1)\n",
        "\n",
        "            obs, rewards, terminated, truncated, _ = train_env.step(actions)\n",
        "            active = ~done_mask\n",
        "            cum_rewards[active] += rewards[active]\n",
        "            done_mask |= (terminated | truncated)\n",
        "\n",
        "        # store –CEM minimizes, so negate average reward\n",
        "        results[i] = -cum_rewards.mean()\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TUhtJYNn7Ai"
      },
      "source": [
        "## Helper function - evaluation\n",
        "\n",
        "You don't need to implement this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1nwwEosfvgCv"
      },
      "outputs": [],
      "source": [
        "def animate_policy(\n",
        "    theta: np.ndarray,\n",
        "    env_name: str = \"CartPole-v1\",\n",
        "    max_steps: int = 500,\n",
        "    interval: int = 100,\n",
        "    goal_reward: float = None\n",
        ") -> HTML:\n",
        "    \"\"\"\n",
        "    Render a discrete-action policy for any Gymnasium env and display a reward curve\n",
        "    with an optional goal_reward marker.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    theta : np.ndarray, shape=(obs_dim*action_dim + action_dim,)\n",
        "        Flattened policy parameters.\n",
        "    env_name : str\n",
        "        Gym environment ID.\n",
        "    max_steps : int\n",
        "        Max timesteps to run.\n",
        "    interval : int\n",
        "        Milliseconds between animation frames.\n",
        "    goal_reward : float or None\n",
        "        If provided, draw a horizontal line at this reward and a vertical line\n",
        "        at the first step where cumulative reward ≥ goal_reward.\n",
        "    \"\"\"\n",
        "    # Setup env\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "    assert theta.size == obs_dim*action_dim + action_dim, \\\n",
        "        f\"θ length should be {obs_dim*action_dim + action_dim}, got {theta.size}\"\n",
        "    W = theta[:obs_dim*action_dim].reshape(obs_dim, action_dim)\n",
        "    b = theta[obs_dim*action_dim:]\n",
        "\n",
        "    # Rollout and collect frames & rewards\n",
        "    obs, _ = env.reset()\n",
        "    frames, rewards = [], []\n",
        "    total = 0.0\n",
        "    goal_step = None\n",
        "\n",
        "    for t in range(max_steps):\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        logits = obs.dot(W) + b\n",
        "        action = int(np.argmax(logits))\n",
        "        obs, r, terminated, truncated, _ = env.step(action)\n",
        "        total += r\n",
        "        rewards.append(total)\n",
        "        if goal_reward is not None and goal_step is None and total >= goal_reward:\n",
        "            goal_step = t+1\n",
        "        if terminated or truncated:\n",
        "            frames.append(env.render())\n",
        "            rewards.append(total)\n",
        "            break\n",
        "    env.close()\n",
        "\n",
        "    # Encode frames\n",
        "    uris = []\n",
        "    for f in frames:\n",
        "        img = Image.fromarray(f)\n",
        "        buf = io.BytesIO()\n",
        "        img.save(buf, format=\"PNG\")\n",
        "        uris.append(\"data:image/png;base64,\" + base64.b64encode(buf.getvalue()).decode())\n",
        "\n",
        "    # Plot reward curve with goal markers\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(range(len(rewards)), rewards, marker='o')\n",
        "    if goal_reward is not None:\n",
        "        ax.axhline(goal_reward, color='red', linestyle='--', label=f\"Goal = {goal_reward}\")\n",
        "        if goal_step is not None:\n",
        "            ax.axvline(goal_step, color='green', linestyle='--',\n",
        "                       label=f\"Reached at t={goal_step}\")\n",
        "        ax.legend()\n",
        "    ax.set_title(\"Cumulative Reward\")\n",
        "    ax.set_xlabel(\"Step\")\n",
        "    ax.set_ylabel(\"Total Reward\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"PNG\")\n",
        "    plt.close(fig)\n",
        "    graph_uri = \"data:image/png;base64,\" + base64.b64encode(buf.getvalue()).decode()\n",
        "\n",
        "    # Build HTML\n",
        "    total_frames = len(frames)\n",
        "    w = frames[0].shape[1]\n",
        "    h = frames[0].shape[0]\n",
        "\n",
        "    charset = string.ascii_letters + string.digits\n",
        "    chars = np.array(list(charset))\n",
        "    picks = np.random.choice(chars, size=6)\n",
        "    # Join them into one string\n",
        "    frame_id = ''.join(picks)\n",
        "\n",
        "\n",
        "    html = f\"\"\"\n",
        "    <div style=\"text-align:center;\">\n",
        "      <img id=\"cem_frame_{frame_id}\" width=\"{w}\" height=\"{h}\" style=\"border:1px solid #333\"/>\n",
        "      <p id=\"frame_counter_{frame_id}\">Frame: 1 / {total_frames}</p>\n",
        "      <div>\n",
        "        <img src=\"{graph_uri}\" width=\"400\"/>\n",
        "      </div>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "\n",
        "    js = f\"\"\"\n",
        "    const frames = [{','.join(f\"'{u}'\" for u in uris)}];\n",
        "    let idx = 0, total = frames.length;\n",
        "    const img = document.getElementById('cem_frame_{frame_id}'),\n",
        "          counter = document.getElementById('frame_counter_{frame_id}');\n",
        "    img.src = frames[0];\n",
        "    setInterval(() => {{\n",
        "      idx = (idx + 1) % total;\n",
        "      img.src = frames[idx];\n",
        "      counter.innerText = 'Frame: ' + (idx+1) + ' / ' + total;\n",
        "    }}, {interval});\n",
        "    \"\"\"\n",
        "    display(Javascript(js))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AfNR4_0qglU"
      },
      "source": [
        "# Optimization example : Cartpole\n",
        "- [Gymnasium docs](https://gymnasium.farama.org/v0.29.0/environments/classic_control/cart_pole/)\n",
        "- Takes about 30 sec ~ 1 min to optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "a1DPRX8JogPO",
        "outputId": "0516909f-6fa8-4bd8-ba13-1ce15a52d235"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for *: 'NoneType' and 'NoneType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-348037818318>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# np.array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mcem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCEM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_frac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melite_frac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m best_theta = cem.optimize(\n",
            "\u001b[0;32m<ipython-input-4-d79f1e963359>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dim, bounds, population_size, elite_frac, max_iters, smoothing, seed)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_elite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melite_frac\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'NoneType'"
          ]
        }
      ],
      "source": [
        "train_env = gym.make_vec(\n",
        "    \"CartPole-v1\",\n",
        "    num_envs = 3,\n",
        "    vectorization_mode = \"vector_entry_point\"\n",
        ")\n",
        "obs_dim = train_env.single_observation_space.shape[0]\n",
        "action_dim = train_env.single_action_space.n\n",
        "objective = lambda batch: objective_vec(train_env, batch)\n",
        "\n",
        "\"\"\"\n",
        "You need to define\n",
        "1. Dimension of linear policy weight.\n",
        "    NOTE:\n",
        "        Weight * Observation = Action.\n",
        "        Weight would include bias term. So the dimension of weight should be ...\n",
        "2. Bound where we should search policy\n",
        "3. hyperparameters of CEM. population_size, elite_frace, max_iters, smoothing ratio etc...\n",
        "\"\"\"\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# 1. Dimension of linear policy weight.\n",
        "dim = 30 * obs_dim *  # int\n",
        "# 2. Bound where we should search policy\n",
        "bounds = np.stack([np.ones(dim), -np.ones(dim)]) # np.array\n",
        "# 3. hyperparameters of CEM\n",
        "population_size = 100 # int\n",
        "elite_frac = 0.1 # float\n",
        "max_iters = 100 # int\n",
        "smoothing = 0.5 # float\n",
        "x0 = np.random.randn((population_size, dim)) # np.array\n",
        "\n",
        "cem = CEM(dim=dim, bounds=bounds, population_size=population_size, elite_frac=elite_frac, max_iters=max_iters, smoothing=smoothing)\n",
        "\n",
        "best_theta = cem.optimize(\n",
        "    x0,\n",
        "    objective,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs_dim"
      ],
      "metadata": {
        "id": "Ncn40CEsqMLl",
        "outputId": "cabe0956-4fbc-483a-b570-068a1a2456d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KBF-5rDn7Ak"
      },
      "source": [
        "Test if your policy is working well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqUubxdqvkTg"
      },
      "outputs": [],
      "source": [
        "animate_policy(best_theta, max_steps=500, interval=100, env_name='CartPole-v1', goal_reward=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Om53i3q9Rw"
      },
      "source": [
        "# Optimization example : Lunar Lander\n",
        "- [Gymnasium docs](https://gymnasium.farama.org/v0.29.0/environments/box2d/lunar_lander/)\n",
        "- Takes about 10 min to optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btNYtIWVqPeI"
      },
      "outputs": [],
      "source": [
        "train_env = gym.make_vec(\n",
        "    \"LunarLander-v3\",\n",
        "    num_envs = 3,\n",
        ")\n",
        "\n",
        "obs_dim = train_env.single_observation_space.shape[0]\n",
        "action_dim = train_env.single_action_space.n\n",
        "objective = lambda batch: objective_vec(train_env, batch)\n",
        "\n",
        "\"\"\"\n",
        "You need to define\n",
        "1. Dimension of linear policy weight .\n",
        "    NOTE:\n",
        "        Weight * Observation = Action.\n",
        "        Weight would include bias term. So the dimension of weight should be ...\n",
        "2. Bound where we should search policy\n",
        "3. hyperparameters of CEM. population_size, elite_frace, max_iters, smoothing ratio etc...\n",
        "\"\"\"\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# 1. Dimension of linear policy weight.\n",
        "dim = None # int\n",
        "# 2. Bound where we should search policy\n",
        "bounds = None # np.array\n",
        "# 3. hyperparameters of CEM\n",
        "population_size = None # int\n",
        "elite_frac = None # float\n",
        "max_iters = None # int\n",
        "smoothing = None # float\n",
        "x0 = np.randn # np.array\n",
        "\n",
        "cem = CEM(dim=dim, bounds=bounds, population_size=population_size, elite_frac=elite_frac, max_iters=max_iters, smoothing=smoothing)\n",
        "\n",
        "best_theta = cem.optimize(\n",
        "    x0,\n",
        "    objective,\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulEFnomun7Al"
      },
      "source": [
        "Test if your policy is working well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqOuTL3MtnkB"
      },
      "outputs": [],
      "source": [
        "animate_policy(best_theta, max_steps=500, interval=100, env_name='LunarLander-v3', goal_reward=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ymXvWKhn7Al"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHkdk-Qen7Al"
      },
      "outputs": [],
      "source": [
        "!python -m pip install cma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpr08q6RvSmg"
      },
      "outputs": [],
      "source": [
        "# Having fun with CMA-ES\n",
        "import cma\n",
        "\n",
        "es = cma.CMAEvolutionStrategy(dim * [0], 1.0, {'popsize': 100, 'maxiter':200})\n",
        "while not es.stop():\n",
        "  solutions = es.ask()\n",
        "  res = objective_vec(train_env, np.stack(solutions, axis=0))\n",
        "  es.tell(solutions, res)\n",
        "  es.logger.add()  # write data to disc to be plotted\n",
        "  es.disp()\n",
        "es.result\n",
        "animate_policy(es.result.xbest,\n",
        "               max_steps=500, interval=100, env_name='LunarLander-v3', goal_reward=200)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}